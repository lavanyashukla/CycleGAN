{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "SpaceInvaders - Baseline.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lavanyashukla/CycleGAN/blob/master/baseline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "KfOuv88IHJTV"
      },
      "source": [
        "# Space Invaders\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "o-uAdQfWkjIh",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "cc5f1dd2-20a2-4425-accc-3d5f9c6e3fa7"
      },
      "source": [
        "!pip install wandb -qq"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 1.4MB 9.6MB/s \n",
            "\u001b[K     |████████████████████████████████| 102kB 16.7MB/s \n",
            "\u001b[K     |████████████████████████████████| 102kB 15.5MB/s \n",
            "\u001b[K     |████████████████████████████████| 102kB 14.8MB/s \n",
            "\u001b[K     |████████████████████████████████| 460kB 63.1MB/s \n",
            "\u001b[K     |████████████████████████████████| 71kB 10.5MB/s \n",
            "\u001b[K     |████████████████████████████████| 71kB 12.7MB/s \n",
            "\u001b[?25h  Building wheel for watchdog (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for subprocess32 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for gql (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for shortuuid (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for graphql-core (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ms0B7dnLHJTc",
        "outputId": "35a98c6c-8123-4b91-9297-480b0bfaebbd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 199
        }
      },
      "source": [
        "import numpy as np\n",
        "import gym\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.contrib.layers import flatten, conv2d, fully_connected\n",
        "from collections import deque, Counter\n",
        "import random\n",
        "from datetime import datetime\n",
        "import wandb\n",
        "wandb.init(project=\"qualcomm\")"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://app.wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "wandb: ERROR Not authenticated.  Copy a key from https://app.wandb.ai/authorize\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "API Key: ··········\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "wandb: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
              "                Project page: <a href=\"https://app.wandb.ai/lavanyashukla/qualcomm\" target=\"_blank\">https://app.wandb.ai/lavanyashukla/qualcomm</a><br/>\n",
              "                Run page: <a href=\"https://app.wandb.ai/lavanyashukla/qualcomm/runs/ujk9fass\" target=\"_blank\">https://app.wandb.ai/lavanyashukla/qualcomm/runs/ujk9fass</a><br/>\n",
              "            "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "W&B Run: https://app.wandb.ai/lavanyashukla/qualcomm/runs/ujk9fass"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "collapsed": true,
        "id": "wXDlF6cdHJTj"
      },
      "source": [
        "## Preprocessing - crop images, convert them to 1D black and white image tensors\n",
        "\n",
        "- Image dimensions - (210, 160, 3)\n",
        "- Output dimensions - (88, 80, 1)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "29CSBYkPHJTl",
        "colab": {}
      },
      "source": [
        "color = np.array([210, 164, 74]).mean()\n",
        "\n",
        "def preprocess_frame(obs):\n",
        "    # Crop and resize\n",
        "    img = obs[25:201:2, ::2]\n",
        "\n",
        "    # Convert to greyscale\n",
        "    img = img.mean(axis=2)\n",
        "\n",
        "    # Improve contrast\n",
        "    img[img==color] = 0\n",
        "\n",
        "    # Normalzie image\n",
        "    img = (img - 128) / 128 - 1\n",
        "\n",
        "    # Reshape to 80*80*1\n",
        "    img = img.reshape(88,80)\n",
        "\n",
        "    return img "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Y1qZntlnHJTs"
      },
      "source": [
        "## Initialize gym environment and explore game screens\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "jnbrjWstHJTu",
        "outputId": "a11fc8ae-e354-4e95-cfd6-9738aa72124f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 303
        }
      },
      "source": [
        "env = gym.make(\"SpaceInvaders-v0\")\n",
        "n_outputs = env.action_space.n\n",
        "print(n_outputs)\n",
        "print(env.env.get_action_meanings())\n",
        "\n",
        "observation = env.reset()\n",
        "# Game Screen\n",
        "for i in range(22):\n",
        "  if i > 20:\n",
        "    plt.imshow(observation)\n",
        "    plt.show()\n",
        "  observation, _, _, _ = env.step(1)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "6\n",
            "['NOOP', 'FIRE', 'RIGHT', 'LEFT', 'RIGHTFIRE', 'LEFTFIRE']\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAM4AAAD8CAYAAAA/rZtiAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "_Ma31y4wFpst",
        "outputId": "c4ac73ba-39bc-4f25-8339-ff5654492050",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 268
        }
      },
      "source": [
        "# Preprocessed Game Screen\n",
        "obs_preprocessed = preprocess_frame(observation).reshape(88,80)\n",
        "plt.imshow(obs_preprocessed)\n",
        "plt.show()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAOcAAAD7CAYAAACR4IPAAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nwberf3rWAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "3nq2xAJaSlM6"
      },
      "source": [
        "## Frame stacking and composition\n",
        "\n",
        "Useful in detecting motion in frames - Returns an element wise max sum of 2 frames - a technique from the deepmind paper. This allows the agent to select an action based on the last few screens, not just the last screen."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "fkbOSaSWjODi",
        "colab": {}
      },
      "source": [
        "stack_size = 4\n",
        "\n",
        "# Initialize deque with zero-images one array for each image\n",
        "stacked_frames  =  deque([np.zeros((88,80), dtype=np.int) for i in range(stack_size)], maxlen=4)\n",
        "\n",
        "def stack_frames(stacked_frames, state, is_new_episode):\n",
        "    # Preprocess frame\n",
        "    frame = preprocess_frame(state)\n",
        "    \n",
        "    if is_new_episode:\n",
        "        # Clear stacked_frames\n",
        "        stacked_frames = deque([np.zeros((88,80), dtype=np.int) for i in range(stack_size)], maxlen=4)\n",
        "        \n",
        "        # Copy the same frame 4x for the new episode, apply elementwise maxima\n",
        "        maxframe = np.maximum(frame,frame)\n",
        "        stacked_frames.append(maxframe)\n",
        "        stacked_frames.append(maxframe)\n",
        "        stacked_frames.append(maxframe)\n",
        "        stacked_frames.append(maxframe)\n",
        "        \n",
        "        # Stack frames\n",
        "        stacked_state = np.stack(stacked_frames, axis=2)\n",
        "    else:\n",
        "        # Fetch rightmost element as the deque append adds t to the right\n",
        "        maxframe=np.maximum(stacked_frames[-1],frame)\n",
        "        # Append frame to deque - removes the oldest frame\n",
        "        stacked_frames.append(maxframe)\n",
        "\n",
        "        # Build the stacked state (first dimension specifies different frames)\n",
        "        stacked_state = np.stack(stacked_frames, axis=2) \n",
        "    \n",
        "    return stacked_state, stacked_frames"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "zTRBSM53HJUM"
      },
      "source": [
        "## Define hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "r1iZveLKHJUN",
        "colab": {}
      },
      "source": [
        "wandb.config.num_episodes = 100\n",
        "wandb.config.batch_size = 48\n",
        "wandb.config.discount_factor = 0.97\n",
        "wandb.config.learning_rate = 0.001\n",
        "\n",
        "copy_steps = 100\n",
        "global_step = 0\n",
        "steps_train = 4\n",
        "start_steps = 2000\n",
        "input_shape = (None, 88, 80, 1)\n",
        "# This is the modified shape with stacked frames\n",
        "X_shape = (None, 88, 80, 4)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "collapsed": true,
        "id": "t5lhlUdqHJT2"
      },
      "source": [
        "## Define deep Q network\n",
        "Has 4 conv layers and a fully connected layer that outputs the probabilities of taking each action in the game space."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "J4Cco6huHJT4",
        "colab": {}
      },
      "source": [
        "tf.compat.v1.reset_default_graph()\n",
        "def q_network(X, name_scope):\n",
        "    \n",
        "    initializer = tf.compat.v1.keras.initializers.VarianceScaling(scale=2.0)\n",
        "\n",
        "    with tf.compat.v1.variable_scope(name_scope) as scope: \n",
        "        layer_1 = conv2d(X, num_outputs=32, kernel_size=(8,8), stride=4, padding='SAME', weights_initializer=initializer) \n",
        "        tf.compat.v1.summary.histogram('layer_1',layer_1)\n",
        "        \n",
        "        layer_2 = conv2d(layer_1, num_outputs=64, kernel_size=(4,4), stride=2, padding='SAME', weights_initializer=initializer)\n",
        "        tf.compat.v1.summary.histogram('layer_2',layer_2)\n",
        "        \n",
        "        layer_3 = conv2d(layer_2, num_outputs=64, kernel_size=(3,3), stride=1, padding='SAME', weights_initializer=initializer)\n",
        "        tf.compat.v1.summary.histogram('layer_3',layer_3)\n",
        "\n",
        "        layer_4 = conv2d(layer_2, num_outputs=64, kernel_size=(3,3), stride=1, padding='SAME', weights_initializer=initializer)\n",
        "        tf.compat.v1.summary.histogram('layer_4',layer_4)\n",
        "        \n",
        "        flat = flatten(layer_4)\n",
        "\n",
        "        fc = fully_connected(flat, num_outputs=128, weights_initializer=initializer)\n",
        "        tf.compat.v1.summary.histogram('fc',fc)\n",
        "\n",
        "        output = fully_connected(fc, num_outputs=n_outputs, activation_fn=None, weights_initializer=initializer)\n",
        "        tf.compat.v1.summary.histogram('output',output)\n",
        "\n",
        "        vars = {v.name[len(scope.name):]: v for v in tf.compat.v1.get_collection(key=tf.compat.v1.GraphKeys.TRAINABLE_VARIABLES, scope=scope.name)} \n",
        "        return vars, output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "fsuM5sy3HJT9"
      },
      "source": [
        "## Implement epsilon-greedy policy\n",
        "\n",
        "Set epsilon values to trade between exploratory (random) and next best actions. It starts out with only (epsilon=) 5% of actions being random and slowly increases this number.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "pSgZ4PHaHJT_",
        "colab": {}
      },
      "source": [
        "epsilon = 0.5\n",
        "eps_min = 0.05\n",
        "eps_max = 1.0\n",
        "eps_decay_steps = 500000\n",
        "\n",
        "def epsilon_greedy(action, step):\n",
        "    p = np.random.random(1).squeeze() #1D entries returned using squeeze\n",
        "    epsilon = max(eps_min, eps_max - (eps_max-eps_min) * step/eps_decay_steps) #Decaying policy with more steps\n",
        "    if p< epsilon:\n",
        "        return np.random.randint(n_outputs)\n",
        "    else:\n",
        "        return action"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "uK5Ev_r-HJUD"
      },
      "source": [
        "## Evaluate current policy and store variables in a buffer to use during mini batch training\n",
        "\n",
        "This stores the agent's state, action, and reward information in the experience replay buffer. We use this buffer to update our network weights while training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "_DU5VHc-HJUE",
        "colab": {}
      },
      "source": [
        "buffer_len = 20000\n",
        "exp_buffer = deque(maxlen=buffer_len)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "em_AW8I7HJUJ",
        "colab": {}
      },
      "source": [
        "# sample experiences from the memory according to batches\n",
        "def sample_memories(batch_size):\n",
        "    perm_batch = np.random.permutation(len(exp_buffer))[:batch_size]\n",
        "    mem = np.array(exp_buffer)[perm_batch]\n",
        "    return mem[:,0], mem[:,1], mem[:,2], mem[:,3], mem[:,4]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "S-JwepmXHJUQ",
        "colab": {}
      },
      "source": [
        "logdir = 'logs'\n",
        "tf.compat.v1.reset_default_graph()\n",
        "\n",
        "# placeholder for our input i.e game state\n",
        "X = tf.compat.v1.placeholder(tf.float32, shape=X_shape)\n",
        "\n",
        "# toggle training\n",
        "in_training_mode = tf.compat.v1.placeholder(tf.bool)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "B3BS7lsPHJUT"
      },
      "source": [
        "## Build the primary and target Q networks\n",
        "\n",
        "This lets training and data generation happen at the same time."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ibqY86h4HJUU",
        "outputId": "91d78b61-75ea-4a73-c975-ffc64beef714",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "source": [
        "# primary Q network - and generates Q values for all the actions in the state\n",
        "mainQ, mainQ_outputs = q_network(X, 'mainQ')\n",
        "\n",
        "# target Q network\n",
        "targetQ, targetQ_outputs = q_network(X, 'targetQ')\n"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/contrib/layers/python/layers/layers.py:1057: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `layer.__call__` method instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/contrib/layers/python/layers/layers.py:1634: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.flatten instead.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ypgPvOWjHJUW",
        "colab": {}
      },
      "source": [
        "# placeholder for action values\n",
        "X_action = tf.compat.v1.placeholder(tf.int32, shape=(None,))\n",
        "Q_action = tf.reduce_sum(input_tensor=targetQ_outputs * tf.one_hot(X_action, n_outputs), axis=-1, keepdims=True)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Qdr6lsjmHJU1",
        "colab": {}
      },
      "source": [
        "# Copy primary Q network params to target Q network\n",
        "copy_op = [tf.compat.v1.assign(main_name, targetQ[var_name]) for var_name, main_name in mainQ.items()]\n",
        "copy_target_to_main = tf.group(*copy_op)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "9Ghb6bUyHJU5"
      },
      "source": [
        "## Define the loss and optimizer\n",
        "\n",
        "Loss = (target action (action with the highest value) - predicted action)^2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "jBHokSlGHJU6",
        "colab": {}
      },
      "source": [
        "# define a placeholder for our output i.e action\n",
        "y = tf.compat.v1.placeholder(tf.float32, shape=(None,1))\n",
        "\n",
        "# now we calculate the loss which is the difference between actual value and predicted value\n",
        "loss = tf.reduce_mean(input_tensor=tf.square(y - Q_action))\n",
        "\n",
        "# we use adam optimizer for minimizing the loss\n",
        "optimizer = tf.compat.v1.train.AdamOptimizer(wandb.config.learning_rate)\n",
        "training_op = optimizer.minimize(loss)\n",
        "\n",
        "init = tf.compat.v1.global_variables_initializer()\n",
        "\n",
        "loss_summary = tf.compat.v1.summary.scalar('LOSS', loss)\n",
        "merge_summary = tf.compat.v1.summary.merge_all()\n",
        "file_writer = tf.compat.v1.summary.FileWriter(logdir, tf.compat.v1.get_default_graph())\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "mwGRru2gOSGd"
      },
      "source": [
        "## Render gameplay in the gym env"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "d3896cd2-58cd-4ada-da90-93c82849f92c",
        "id": "WpWWZ9_AxFxz",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "# Visualize in colab\n",
        "\n",
        "# Install dependencies first for graphics visualization within Colaboratory\n",
        "!pip install gym pyvirtualdisplay > /dev/null 2>&1\n",
        "!apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1\n",
        "\n",
        "!apt-get update > /dev/null 2>&1\n",
        "!apt-get install cmake > /dev/null 2>&1\n",
        "!pip install --upgrade setuptools 2>&1\n",
        "!pip install ez_setup > /dev/null 2>&1\n",
        "\n",
        "from gym import logger as gymlogger\n",
        "from gym.wrappers import Monitor\n",
        "gymlogger.set_level(40)\n",
        "import numpy as np\n",
        "import random\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import math\n",
        "import glob\n",
        "import io\n",
        "import base64\n",
        "from IPython.display import HTML\n",
        "from IPython import display as ipythondisplay\n",
        "from pyvirtualdisplay import Display\n",
        "display = Display(visible=0, size=(1400, 900))\n",
        "display.start()"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already up-to-date: setuptools in /usr/local/lib/python3.6/dist-packages (45.1.0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "xdpyinfo was not found, X start can not be checked! Please install xdpyinfo!\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Display cmd_param=['Xvfb', '-br', '-nolisten', 'tcp', '-screen', '0', '1400x900x24', ':1001'] cmd=['Xvfb', '-br', '-nolisten', 'tcp', '-screen', '0', '1400x900x24', ':1001'] oserror=None return_code=None stdout=\"None\" stderr=\"None\" timeout_happened=False>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "8HOaL7rfHJU_"
      },
      "source": [
        "## Train the network\n",
        "\n",
        "Takes as input a stack of greyscale images (88,88,4), generates a probability distribution of next actions.\n",
        "\n",
        "Uses the epsilon-greedy policy to tradeoff between next ebst action and random exploratory action and select the next action.\n",
        "\n",
        "For a predefined # of steps - Feed next action into the network and get the next state and rewards, store them into the buffer. Pick next action using epsilon-greedy policy and repeat. Perform gradient descent in order to minimize the loss."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "VWMSuB2cHJVA",
        "outputId": "47e3504b-5200-43b2-c034-6cbbf8c6f63b",
        "scrolled": false,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        }
      },
      "source": [
        "with tf.compat.v1.Session() as sess:\n",
        "    init.run()\n",
        "    \n",
        "    # for each episode\n",
        "    history = []\n",
        "    for i in range(wandb.config.num_episodes):\n",
        "        done = False\n",
        "        environment = Monitor(env, './video', force=True)\n",
        "        obs = environment.reset()\n",
        "        epoch = 0\n",
        "        episodic_reward = 0\n",
        "        actions_counter = Counter() \n",
        "        episodic_loss = []\n",
        "        \n",
        "        # begin stacking frames\n",
        "        obs,stacked_frames= stack_frames(stacked_frames,obs,True)\n",
        "\n",
        "        # while the state is not the terminal state\n",
        "        while not done:\n",
        "\n",
        "           # Data generation using the untrained network\n",
        "            # feed the game screen and get the Q values for each action,  FEED THE NETWORK BY CALLING THE OUTPUT LAYER    \n",
        "            actions = mainQ_outputs.eval(feed_dict={X:[obs], in_training_mode:False})\n",
        "\n",
        "            # get the action\n",
        "            action = np.argmax(actions, axis=-1)\n",
        "            actions_counter[str(action)] += 1 \n",
        "\n",
        "            # select the action using epsilon greedy policy\n",
        "            action = epsilon_greedy(action, global_step)\n",
        "            environment.render()\n",
        "            \n",
        "            # now perform the action and move to the next state, next_obs, receive reward\n",
        "            next_obs, reward, done, _ = environment.step(action)\n",
        "\n",
        "            #Begin stacking intra-episode code\n",
        "            next_obs, stacked_frames = stack_frames(stacked_frames, next_obs, False)\n",
        "\n",
        "            # Store this transistion as an experience in the replay buffer! Quite important\n",
        "            exp_buffer.append([obs, action, next_obs, reward, done])\n",
        "            \n",
        "            # After certain steps, we train our Q network with samples from the experience replay buffer\n",
        "            if global_step % steps_train == 0 and global_step > start_steps:\n",
        "                #Our buffer should already contain everything preprocessed and stacked\n",
        "                # sample experience, mem[:,0], mem[:,1], mem[:,2], mem[:,3], mem[:,4]\n",
        "                o_obs, o_act, o_next_obs, o_rew, o_done = sample_memories(wandb.config.batch_size)\n",
        "\n",
        "                # states\n",
        "                o_obs = [x for x in o_obs]\n",
        "\n",
        "                # next states\n",
        "                o_next_obs = [x for x in o_next_obs]\n",
        "\n",
        "                # next actions\n",
        "                next_act = mainQ_outputs.eval(feed_dict={X:o_next_obs, in_training_mode:False})\n",
        "\n",
        "                # discounted reward: these are our Y-values\n",
        "                y_batch = o_rew + wandb.config.discount_factor * np.max(next_act, axis=-1) * (1-o_done) \n",
        "\n",
        "                # merge all summaries and write to the file\n",
        "                mrg_summary = merge_summary.eval(feed_dict={X:o_obs, y:np.expand_dims(y_batch, axis=-1), X_action:o_act, in_training_mode:False})\n",
        "                file_writer.add_summary(mrg_summary, global_step)\n",
        "\n",
        "                # to calculate the loss, we run the previously defined functions mentioned while feeding inputs\n",
        "                train_loss, _ = sess.run([loss, training_op], feed_dict={X:o_obs, y:np.expand_dims(y_batch, axis=-1), X_action:o_act, in_training_mode:True})\n",
        "                episodic_loss.append(train_loss)\n",
        "\n",
        "                # ;og loss into wandb\n",
        "                wandb.log({'loss': train_loss})\n",
        "            \n",
        "            # after some interval we copy our main Q network weights to target Q network\n",
        "            if (global_step+1) % copy_steps == 0 and global_step > start_steps:\n",
        "                copy_target_to_main.run()\n",
        "                \n",
        "            obs = next_obs\n",
        "            epoch += 1\n",
        "            global_step += 1\n",
        "            episodic_reward += reward\n",
        "\n",
        "        next_obs=np.zeros(obs.shape)\n",
        "        exp_buffer.append([obs, action, next_obs, reward, done])\n",
        "        obs= environment.reset()\n",
        "        obs,stacked_frames= stack_frames(stacked_frames,obs,True) \n",
        "        \n",
        "        history.append(episodic_reward)\n",
        "\n",
        "        # log reward into wandb\n",
        "        wandb.log({'reward': episodic_reward})\n",
        "        environment.close()\n",
        "\n",
        "        # render video\n",
        "        mp4list = glob.glob('video/*.mp4')\n",
        "        if len(mp4list) > 0:\n",
        "          print(len(mp4list))\n",
        "          mp4 = mp4list[-1]\n",
        "          video = io.open(mp4, 'r+b').read()\n",
        "          encoded = base64.b64encode(video)\n",
        "\n",
        "          # log video into wandb\n",
        "          wandb.log({\"gameplays\": wandb.Video(mp4, fps=4, format=\"gif\")})\n",
        "          ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay \n",
        "                      loop controls style=\"height: 400px;\">\n",
        "                      <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
        "                  </video>'''.format(encoded.decode('ascii'))))\n",
        "        else: \n",
        "          print(\"Could not find video\")\n",
        "        print('Epochs per episode:', epoch, 'Episode Reward:', episodic_reward,\"Episode number:\", len(history))\n",
        "    \n",
        "\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-00d477b9144b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0minit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;31m# for each episode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'tf' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "OKk22m-0CbEs",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}